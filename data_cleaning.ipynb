{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T21:04:22.188750Z",
     "start_time": "2025-12-08T21:04:20.789891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "original_ds = (\n",
    "    load_dataset('jziebura/polish_youth_slang_classification')\n",
    "    .filter(lambda x: x['znaczenie wyrazów slangowych'] is not None)\n",
    "    .map(\n",
    "        lambda x: {\n",
    "            'word': x['słowo slangowe'].strip(),\n",
    "            'meaning': x['znaczenie wyrazów slangowych'].strip(),\n",
    "            'text': x['tekst'].strip()}\n",
    "    )\n",
    ")"
   ],
   "id": "85da59875f4c06cd",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T21:04:22.204968Z",
     "start_time": "2025-12-08T21:04:22.192297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds = (\n",
    "    original_ds\n",
    "    .map(lambda x: {'text': \" - \".join([x['word'], x['meaning']])})\n",
    "    .rename_column('sentyment', 'label')\n",
    "    .class_encode_column('label')\n",
    ")"
   ],
   "id": "bad6077fac8c1bd3",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T21:04:22.251327Z",
     "start_time": "2025-12-08T21:04:22.240629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.concat(ds[split].to_pandas() for split in ds.keys())"
   ],
   "id": "56161f9f704ddb7e",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T21:13:44.381907Z",
     "start_time": "2025-12-08T21:13:43.677656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "AutoTokenizer.from_pretrained('gpt2').vocab_size"
   ],
   "id": "85859549e8753ad8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T21:04:26.365562Z",
     "start_time": "2025-12-08T21:04:26.362353Z"
    }
   },
   "cell_type": "code",
   "source": "df['label'].value_counts()",
   "id": "2b6fc56577138fad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    2767\n",
       "0    1561\n",
       "2    1093\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T18:08:41.188700Z",
     "start_time": "2025-12-08T18:08:41.186867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_initial_model_data(texts: np.ndarray, labels: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "    clf = LogisticRegression(max_iter=1000, random_state=0)\n",
    "    pred_probs = cross_val_predict(\n",
    "        clf,\n",
    "        embeddings,\n",
    "        labels,\n",
    "        cv=5,\n",
    "        method=\"predict_proba\",\n",
    "    )\n",
    "\n",
    "    return embeddings, pred_probs"
   ],
   "id": "107881c8da4cdbe6",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T18:08:41.234708Z",
     "start_time": "2025-12-08T18:08:41.231260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Callable\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def merge_duplicate_sets(df, merge_key: str):\n",
    "    \"\"\"Generate group keys for each row, then merge intersecting sets.\n",
    "\n",
    "    :param df: DataFrame with columns 'is_near_duplicate_issue' and 'near_duplicate_sets'\n",
    "    :param merge_key: Name of the column to store the merged sets\n",
    "    \"\"\"\n",
    "\n",
    "    df[merge_key] = df.apply(construct_group_key, axis=1)\n",
    "    merged_sets = consolidate_sets(df[merge_key].tolist())\n",
    "    df[merge_key] = df[merge_key].map(\n",
    "        lambda x: next(s for s in merged_sets if x.issubset(s))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def construct_group_key(row):\n",
    "    \"\"\"Convert near_duplicate_sets into a frozenset and include the row's own index.\"\"\"\n",
    "    return frozenset(row['near_duplicate_sets']).union({row.name})\n",
    "\n",
    "def consolidate_sets(sets_list):\n",
    "    \"\"\"Merge sets if they intersect.\"\"\"\n",
    "\n",
    "    # Convert the input list of frozensets to a list of mutable sets\n",
    "    sets_list = [set(item) for item in sets_list]\n",
    "\n",
    "    # A flag to keep track of whether any sets were merged in the current iteration\n",
    "    merged = True\n",
    "\n",
    "    # Continue the merging process as long as we have merged some sets in the previous iteration\n",
    "    while merged:\n",
    "        merged = False\n",
    "        new_sets = []\n",
    "\n",
    "        # Iterate through each set in our list\n",
    "        for current_set in sets_list:\n",
    "            # Skip empty sets\n",
    "            if not current_set:\n",
    "                continue\n",
    "\n",
    "            # Find all sets that have an intersection with the current set\n",
    "            intersecting_sets = [s for s in sets_list if s & current_set]\n",
    "\n",
    "            # If more than one set intersects, set the merged flag to True\n",
    "            if len(intersecting_sets) > 1:\n",
    "                merged = True\n",
    "\n",
    "            # Merge all intersecting sets into one set\n",
    "            merged_set = set().union(*intersecting_sets)\n",
    "            new_sets.append(merged_set)\n",
    "\n",
    "            # Empty the sets we've merged to prevent them from being processed again\n",
    "            for s in intersecting_sets:\n",
    "                sets_list[sets_list.index(s)] = set()\n",
    "\n",
    "        # Replace the original sets list with the new list of merged sets\n",
    "        sets_list = new_sets\n",
    "\n",
    "    # Convert the merged sets back to frozensets for the output\n",
    "    return [frozenset(item) for item in sets_list]\n",
    "\n",
    "def lowest_score_strategy(sub_df):\n",
    "    \"\"\"Keep the row with the lowest near_duplicate_score.\"\"\"\n",
    "    return sub_df['near_duplicate_score'].idxmin()\n",
    "\n",
    "\n",
    "def filter_near_duplicates(data: pd.DataFrame, strategy_fn: Callable = lowest_score_strategy, **strategy_kwargs):\n",
    "    \"\"\"\n",
    "    Given a dataframe with columns 'is_near_duplicate_issue' and 'near_duplicate_sets',\n",
    "    return a series of boolean values where True indicates the rows to be removed.\n",
    "    The strategy_fn determines which rows to keep within each near_duplicate_set.\n",
    "\n",
    "    :param data: DataFrame with is_near_duplicate_issue and near_duplicate_sets columns\n",
    "    :param strategy_fn: Function to determine which rows to keep within each near_duplicate_set\n",
    "    :return: Series of boolean values where True indicates rows to be removed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter out rows where 'is_near_duplicate_issue' is True to get potential duplicates\n",
    "    duplicate_rows = data.query(\"is_near_duplicate_issue\").copy()\n",
    "\n",
    "    # Generate group keys for each row and merge intersecting sets\n",
    "    group_key = \"sets\"\n",
    "    duplicate_rows = merge_duplicate_sets(duplicate_rows, merge_key=group_key)\n",
    "\n",
    "    # Use the strategy function to determine the indices of the rows to keep for each group\n",
    "    to_keep_indices = duplicate_rows.groupby(group_key).apply(strategy_fn, **strategy_kwargs).explode().values\n",
    "\n",
    "    # Produce a boolean series indicating which rows should be removed\n",
    "    to_remove = ~data.index.isin(to_keep_indices)\n",
    "\n",
    "    return to_remove"
   ],
   "id": "aecc759d96d7e080",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T18:09:44.674052Z",
     "start_time": "2025-12-08T18:09:07.586900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from cleanlab import Datalab\n",
    "\n",
    "for i in range(10):\n",
    "    texts = df[\"text\"].values\n",
    "    labels = df[\"label\"].values\n",
    "\n",
    "    embeddings, pred_probs = get_initial_model_data(texts, labels)\n",
    "    lab = Datalab(df, label_name='label', task='classification')\n",
    "    lab.find_issues(pred_probs=pred_probs, features=embeddings)\n",
    "    lab.report()\n",
    "\n",
    "    df_deduplicated = df.copy()\n",
    "\n",
    "    duplicate_issues = lab.get_issues(\"near_duplicate\")\n",
    "\n",
    "    label_issues = lab.get_issues(\"label\")\n",
    "    label_issues = label_issues[label_issues[\"is_label_issue\"]]\n",
    "\n",
    "    duplicates_count = len(duplicate_issues[duplicate_issues[\"is_near_duplicate_issue\"] == True])\n",
    "    label_issues_count = len(label_issues)\n",
    "\n",
    "    if duplicates_count:\n",
    "        print(filter_near_duplicates(duplicate_issues) )\n",
    "        duplicate_issues = duplicate_issues[filter_near_duplicates(duplicate_issues) & duplicate_issues[\"is_near_duplicate_issue\"]]\n",
    "        df_deduplicated = df_deduplicated.drop(df_deduplicated[df_deduplicated.index.isin(duplicate_issues.index)].index, axis='index')\n",
    "\n",
    "    if label_issues_count < 40 and duplicates_count < 40:\n",
    "        break\n",
    "\n",
    "    label_issues = label_issues[label_issues.index.isin(df_deduplicated.index)]\n",
    "    idxs = label_issues.index.tolist()\n",
    "    pred_labels = label_issues[\"predicted_label\"]\n",
    "\n",
    "    df_fixed = df_deduplicated.copy()\n",
    "    df_fixed.loc[idxs, \"label\"] = pred_labels\n",
    "\n",
    "    df = df_fixed.reset_index(drop=True)"
   ],
   "id": "9849a193dffe6cf8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/170 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b60f934b1e74caa9980cda1d3e346bc"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding null issues ...\n",
      "Finding label issues ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding outlier issues ...\n",
      "Finding near_duplicate issues ...\n",
      "Finding non_iid issues ...\n",
      "Finding class_imbalance issues ...\n",
      "Finding underperforming_group issues ...\n",
      "\n",
      "Audit complete. 2959 issues found in the dataset.\n",
      "Dataset Information: num_examples: 5421, num_classes: 3\n",
      "\n",
      "Here is a summary of various issues found in your data:\n",
      "\n",
      "    issue_type  num_issues\n",
      "         label        1469\n",
      "near_duplicate        1450\n",
      "       outlier          40\n",
      "\n",
      "Learn about each issue: https://docs.cleanlab.ai/stable/cleanlab/datalab/guide/issue_type_description.html\n",
      "See which examples in your dataset exhibit each issue via: `datalab.get_issues(<ISSUE_NAME>)`\n",
      "\n",
      "Data indices corresponding to top examples of each issue are shown below.\n",
      "\n",
      "\n",
      "----------------------- label issues -----------------------\n",
      "\n",
      "About this issue:\n",
      "\tExamples whose given label is estimated to be potentially incorrect\n",
      "    (e.g. due to annotation error) are flagged as having label issues.\n",
      "    \n",
      "\n",
      "Number of examples with this issue: 1469\n",
      "Overall dataset quality in terms of this issue: 0.7499\n",
      "\n",
      "Examples representing most severe instances of this issue:\n",
      "      is_label_issue  label_score  given_label  predicted_label\n",
      "2257            True     0.015970            2                1\n",
      "4391            True     0.017155            2                1\n",
      "3953            True     0.023888            2                1\n",
      "184             True     0.025827            2                1\n",
      "3128            True     0.029909            2                1\n",
      "\n",
      "\n",
      "------------------ near_duplicate issues -------------------\n",
      "\n",
      "About this issue:\n",
      "\tA (near) duplicate issue refers to two or more examples in\n",
      "    a dataset that are extremely similar to each other, relative\n",
      "    to the rest of the dataset.  The examples flagged with this issue\n",
      "    may be exactly duplicated, or lie atypically close together when\n",
      "    represented as vectors (i.e. feature embeddings).\n",
      "    \n",
      "\n",
      "Number of examples with this issue: 1450\n",
      "Overall dataset quality in terms of this issue: 0.4847\n",
      "\n",
      "Examples representing most severe instances of this issue:\n",
      "      is_near_duplicate_issue  near_duplicate_score                                                                                                                     near_duplicate_sets  distance_to_nearest_neighbor\n",
      "1011                     True                   0.0                                                                                         [780, 1485, 2111, 3911, 3968, 4335, 4484, 5323]                           0.0\n",
      "1240                     True                   0.0                                                    [1043, 1163, 1421, 1446, 2145, 2729, 2730, 3117, 3319, 3673, 3831, 4035, 5163, 5289]                           0.0\n",
      "1239                     True                   0.0                                                                                                                            [3206, 3330]                           0.0\n",
      "2699                     True                   0.0  [69, 281, 655, 1237, 1869, 1929, 1995, 2771, 2859, 2901, 3011, 3228, 3357, 3422, 3494, 3769, 3950, 3960, 4031, 4347, 4417, 4670, 5113]                           0.0\n",
      "1237                     True                   0.0  [69, 281, 655, 1869, 1929, 1995, 2699, 2771, 2859, 2901, 3011, 3228, 3357, 3422, 3494, 3769, 3950, 3960, 4031, 4347, 4417, 4670, 5113]                           0.0\n",
      "\n",
      "\n",
      "---------------------- outlier issues ----------------------\n",
      "\n",
      "About this issue:\n",
      "\tExamples that are very different from the rest of the dataset \n",
      "    (i.e. potentially out-of-distribution or rare/anomalous instances).\n",
      "    \n",
      "\n",
      "Number of examples with this issue: 40\n",
      "Overall dataset quality in terms of this issue: 0.4449\n",
      "\n",
      "Examples representing most severe instances of this issue:\n",
      "      is_outlier_issue  outlier_score\n",
      "2861              True       0.079430\n",
      "3150              True       0.084480\n",
      "4150              True       0.101065\n",
      "5414              True       0.101876\n",
      "1494              True       0.103182\n",
      "[ True  True  True ...  True  True  True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/139 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6c8a02737e944ae29a89645468c15657"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding null issues ...\n",
      "Finding label issues ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding outlier issues ...\n",
      "Finding near_duplicate issues ...\n",
      "Finding non_iid issues ...\n",
      "Finding class_imbalance issues ...\n",
      "Finding underperforming_group issues ...\n",
      "\n",
      "Audit complete. 983 issues found in the dataset.\n",
      "Dataset Information: num_examples: 4440, num_classes: 3\n",
      "\n",
      "Here is a summary of various issues found in your data:\n",
      "\n",
      "    issue_type  num_issues\n",
      "         label         525\n",
      "near_duplicate         395\n",
      "       outlier          62\n",
      "       non_iid           1\n",
      "\n",
      "Learn about each issue: https://docs.cleanlab.ai/stable/cleanlab/datalab/guide/issue_type_description.html\n",
      "See which examples in your dataset exhibit each issue via: `datalab.get_issues(<ISSUE_NAME>)`\n",
      "\n",
      "Data indices corresponding to top examples of each issue are shown below.\n",
      "\n",
      "\n",
      "----------------------- label issues -----------------------\n",
      "\n",
      "About this issue:\n",
      "\tExamples whose given label is estimated to be potentially incorrect\n",
      "    (e.g. due to annotation error) are flagged as having label issues.\n",
      "    \n",
      "\n",
      "Number of examples with this issue: 525\n",
      "Overall dataset quality in terms of this issue: 0.9142\n",
      "\n",
      "Examples representing most severe instances of this issue:\n",
      "      is_label_issue  label_score  given_label  predicted_label\n",
      "3838            True     0.003776            0                1\n",
      "3885            True     0.004517            0                1\n",
      "4421            True     0.005111            2                1\n",
      "4295            True     0.005796            0                1\n",
      "3998            True     0.006614            0                1\n",
      "\n",
      "\n",
      "------------------ near_duplicate issues -------------------\n",
      "\n",
      "About this issue:\n",
      "\tA (near) duplicate issue refers to two or more examples in\n",
      "    a dataset that are extremely similar to each other, relative\n",
      "    to the rest of the dataset.  The examples flagged with this issue\n",
      "    may be exactly duplicated, or lie atypically close together when\n",
      "    represented as vectors (i.e. feature embeddings).\n",
      "    \n",
      "\n",
      "Number of examples with this issue: 395\n",
      "Overall dataset quality in terms of this issue: 0.5739\n",
      "\n",
      "Examples representing most severe instances of this issue:\n",
      "      is_near_duplicate_issue  near_duplicate_score                                         near_duplicate_sets  distance_to_nearest_neighbor\n",
      "3600                     True                   0.0                                                 [699, 4354]                           0.0\n",
      "3976                     True                   0.0                                                      [4121]                           0.0\n",
      "3974                     True                   0.0  [38, 3675, 3972, 4158, 4321, 3490, 4288, 4274, 3875, 4026]                           0.0\n",
      "3972                     True                   0.0  [38, 3675, 3974, 4158, 4321, 3490, 4288, 4274, 3875, 4026]                           0.0\n",
      "257                      True                   0.0                                                      [3499]                           0.0\n",
      "\n",
      "\n",
      "---------------------- outlier issues ----------------------\n",
      "\n",
      "About this issue:\n",
      "\tExamples that are very different from the rest of the dataset \n",
      "    (i.e. potentially out-of-distribution or rare/anomalous instances).\n",
      "    \n",
      "\n",
      "Number of examples with this issue: 62\n",
      "Overall dataset quality in terms of this issue: 0.3750\n",
      "\n",
      "Examples representing most severe instances of this issue:\n",
      "      is_outlier_issue  outlier_score\n",
      "2358              True       0.093165\n",
      "2586              True       0.099046\n",
      "1281              True       0.112511\n",
      "3337              True       0.117131\n",
      "4433              True       0.118010\n",
      "\n",
      "\n",
      "---------------------- non_iid issues ----------------------\n",
      "\n",
      "About this issue:\n",
      "\tWhether the dataset exhibits statistically significant\n",
      "    violations of the IID assumption like:\n",
      "    changepoints or shift, drift, autocorrelation, etc.\n",
      "    The specific violation considered is whether the\n",
      "    examples are ordered such that almost adjacent examples\n",
      "    tend to have more similar feature values.\n",
      "    \n",
      "\n",
      "Number of examples with this issue: 1\n",
      "Overall dataset quality in terms of this issue: 0.0025\n",
      "\n",
      "Examples representing most severe instances of this issue:\n",
      "      is_non_iid_issue  non_iid_score\n",
      "38                True       0.614658\n",
      "1346             False       0.620908\n",
      "78               False       0.622689\n",
      "785              False       0.631112\n",
      "1214             False       0.646434\n",
      "\n",
      "Additional Information: \n",
      "p-value: 0.0024802876050839416\n",
      "[ True  True  True ...  True  True  True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/131 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5fd0c67c602f4e398c5a02015b270b7f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding null issues ...\n",
      "Finding label issues ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding outlier issues ...\n",
      "Finding near_duplicate issues ...\n",
      "Finding non_iid issues ...\n",
      "Finding class_imbalance issues ...\n",
      "Finding underperforming_group issues ...\n",
      "\n",
      "Audit complete. 143 issues found in the dataset.\n",
      "Dataset Information: num_examples: 4181, num_classes: 3\n",
      "\n",
      "Here is a summary of various issues found in your data:\n",
      "\n",
      "    issue_type  num_issues\n",
      "         label          75\n",
      "       outlier          66\n",
      "near_duplicate           2\n",
      "\n",
      "Learn about each issue: https://docs.cleanlab.ai/stable/cleanlab/datalab/guide/issue_type_description.html\n",
      "See which examples in your dataset exhibit each issue via: `datalab.get_issues(<ISSUE_NAME>)`\n",
      "\n",
      "Data indices corresponding to top examples of each issue are shown below.\n",
      "\n",
      "\n",
      "----------------------- label issues -----------------------\n",
      "\n",
      "About this issue:\n",
      "\tExamples whose given label is estimated to be potentially incorrect\n",
      "    (e.g. due to annotation error) are flagged as having label issues.\n",
      "    \n",
      "\n",
      "Number of examples with this issue: 75\n",
      "Overall dataset quality in terms of this issue: 0.9909\n",
      "\n",
      "Examples representing most severe instances of this issue:\n",
      "      is_label_issue  label_score  given_label  predicted_label\n",
      "55              True     0.036795            2                1\n",
      "160             True     0.055591            2                1\n",
      "2753            True     0.057943            2                1\n",
      "921             True     0.058575            2                1\n",
      "3407            True     0.066049            2                1\n",
      "\n",
      "\n",
      "---------------------- outlier issues ----------------------\n",
      "\n",
      "About this issue:\n",
      "\tExamples that are very different from the rest of the dataset \n",
      "    (i.e. potentially out-of-distribution or rare/anomalous instances).\n",
      "    \n",
      "\n",
      "Number of examples with this issue: 66\n",
      "Overall dataset quality in terms of this issue: 0.3641\n",
      "\n",
      "Examples representing most severe instances of this issue:\n",
      "      is_outlier_issue  outlier_score\n",
      "2349              True       0.096978\n",
      "2577              True       0.103049\n",
      "1276              True       0.116803\n",
      "3327              True       0.119648\n",
      "4174              True       0.122412\n",
      "\n",
      "\n",
      "------------------ near_duplicate issues -------------------\n",
      "\n",
      "About this issue:\n",
      "\tA (near) duplicate issue refers to two or more examples in\n",
      "    a dataset that are extremely similar to each other, relative\n",
      "    to the rest of the dataset.  The examples flagged with this issue\n",
      "    may be exactly duplicated, or lie atypically close together when\n",
      "    represented as vectors (i.e. feature embeddings).\n",
      "    \n",
      "\n",
      "Number of examples with this issue: 2\n",
      "Overall dataset quality in terms of this issue: 0.6144\n",
      "\n",
      "Examples representing most severe instances of this issue:\n",
      "      is_near_duplicate_issue  near_duplicate_score near_duplicate_sets  distance_to_nearest_neighbor\n",
      "1360                     True              0.121023               [319]                      0.033172\n",
      "319                      True              0.121023              [1360]                      0.033172\n",
      "2133                    False              0.122449                  []                      0.033590\n",
      "3757                    False              0.122449                  []                      0.033590\n",
      "3057                    False              0.122909                  []                      0.033725\n",
      "[ True  True  True ...  True  True  True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/131 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1089c7cac50e456d832a73b2c44c01b8"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding null issues ...\n",
      "Finding label issues ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding outlier issues ...\n",
      "Finding near_duplicate issues ...\n",
      "Finding non_iid issues ...\n",
      "Finding class_imbalance issues ...\n",
      "Finding underperforming_group issues ...\n",
      "\n",
      "Audit complete. 91 issues found in the dataset.\n",
      "Dataset Information: num_examples: 4180, num_classes: 3\n",
      "\n",
      "Here is a summary of various issues found in your data:\n",
      "\n",
      "issue_type  num_issues\n",
      "   outlier          66\n",
      "     label          25\n",
      "\n",
      "Learn about each issue: https://docs.cleanlab.ai/stable/cleanlab/datalab/guide/issue_type_description.html\n",
      "See which examples in your dataset exhibit each issue via: `datalab.get_issues(<ISSUE_NAME>)`\n",
      "\n",
      "Data indices corresponding to top examples of each issue are shown below.\n",
      "\n",
      "\n",
      "---------------------- outlier issues ----------------------\n",
      "\n",
      "About this issue:\n",
      "\tExamples that are very different from the rest of the dataset \n",
      "    (i.e. potentially out-of-distribution or rare/anomalous instances).\n",
      "    \n",
      "\n",
      "Number of examples with this issue: 66\n",
      "Overall dataset quality in terms of this issue: 0.3641\n",
      "\n",
      "Examples representing most severe instances of this issue:\n",
      "      is_outlier_issue  outlier_score\n",
      "2348              True       0.096989\n",
      "2576              True       0.103061\n",
      "1276              True       0.116816\n",
      "3326              True       0.119661\n",
      "4173              True       0.122425\n",
      "\n",
      "\n",
      "----------------------- label issues -----------------------\n",
      "\n",
      "About this issue:\n",
      "\tExamples whose given label is estimated to be potentially incorrect\n",
      "    (e.g. due to annotation error) are flagged as having label issues.\n",
      "    \n",
      "\n",
      "Number of examples with this issue: 25\n",
      "Overall dataset quality in terms of this issue: 0.9974\n",
      "\n",
      "Examples representing most severe instances of this issue:\n",
      "      is_label_issue  label_score  given_label  predicted_label\n",
      "275            False     0.060429            2                1\n",
      "2312           False     0.071185            2                1\n",
      "60             False     0.071887            2                1\n",
      "3108           False     0.073843            2                1\n",
      "3602           False     0.075041            2                1\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T18:09:47.230704Z",
     "start_time": "2025-12-08T18:09:47.147510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import zstandard as zstd\n",
    "import json\n",
    "import io\n",
    "def filter_dataset(output_path):\n",
    "    with open(output_path, 'wb') as outfile:\n",
    "        cctx = zstd.ZstdCompressor()\n",
    "        writer = cctx.stream_writer(outfile)\n",
    "\n",
    "        for idx, entry in df_fixed.iterrows():\n",
    "            filtered = {\n",
    "                'word': entry['słowo slangowe'],\n",
    "                'meaning': entry['znaczenie wyrazów slangowych'],\n",
    "                'text': entry[\"tekst\"],\n",
    "                'label': entry.label,\n",
    "            }\n",
    "            writer.write((json.dumps(filtered) + '\\n').encode('utf-8'))\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "filter_dataset('polish_youth_slang_classification_filtered_last.json.zst')"
   ],
   "id": "af1fb29cc04c6b13",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T18:43:33.869234Z",
     "start_time": "2025-12-08T18:43:33.867027Z"
    }
   },
   "cell_type": "code",
   "source": "df['label'].value_counts()",
   "id": "87263ffa1bf5dc11",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    2727\n",
       "0    1191\n",
       "2     262\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
